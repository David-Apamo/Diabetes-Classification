---
title: "Diabetes Classification"
author: "David Apamo"
date: "`r Sys.Date()`"
output: word_document
---

# Introduction

Diabetes is a metabolic condition causing excessive blood sugar levels (MSD Manual). Diabetes occurs when the pancreas either doesn’t produce enough insulin or can’t use the insulin it produces effectively. Insulin is a hormone that regulates blood glucose. The hormone insulin transfers sugar from the blood into the cells for storage or energy use. Patients with the potential of diabetes have to go through a series of tests and examinations to diagnose the disease properly, which are expensive (Tasin, Nabil, Islam, & Khan, 2022). A predictive model that can accurately detect diabetes is therefore needed, as it will help in early detection and treatment/management of the disease.

# Study Objective

The main objective of this study is to develop a predictive model that can accurately predict (with high precision) the likelihood of developing diabetes, and identify the most important predictors of diabetes.

```{r}
# Load packages
suppressMessages(
  {
    library(tidyverse)
    library(janitor)
    library(caret)
    library(mlr)
    library(tidymodels)
    library(pROC)
    library(vip)
    library(parallel)
    library(parallelMap)
  }
)
```
```{r}
# Import data
diabetes <- read_csv("diabetes.txt")
```

```{r}
# View the structure of the dataset
diabetes |> glimpse()
```

The data contains 15000 observations of 10 variables. All the variables are numeric (double).

## Clean the data

```{r}
# Clean variable names
diabetes <- diabetes |> clean_names()
```

```{r}
# Check for missing values
diabetes |> map_dbl(~ sum(is.na(.)))
```

There are no missing values in the data.

```{r}
# Check for duplicated observations
which(duplicated(diabetes))
```

There are no duplicated observations.

```{r}
# Deselect patient_id variable
dbTib <- diabetes |> select(-patient_id)
# Factor the response variable and reverse levels to begin with the positive class
dbTib[["diabetic"]] <- factor(dbTib[["diabetic"]], 
                              levels = rev(c(0,1)), 
                              labels = rev(c("No", "Yes")))
```

## EDA

```{r}
# Generate summary statistics for each and every variable.
dbTib |> summary()
```

* The minimum and first quartile for the number of pregnancies is zero. Male patients definitely have zero number of pregnancies. The median number of pregnancies is 2 and the maximum number of pregnancies is 14.
* The median values for plasma glucose, diastolic blood pressure, triceps thickness, serum insulin, BMI, diabetes pedigree and age are 104, 72, 31, 83, 31.77, 0.2 and 24 respectively.
* Diabetic patients are 5000 while non-diabetic patients are 10,000 in number. There's class imbalance in the data.

```{r}
# Convert the data into longer format for visualization
diabetesUntidy <- gather(dbTib, key = "Variable", value = "Value", - diabetic)
```

```{r}
# Visualize the data
diabetesUntidy |> ggplot(aes(diabetic, as.numeric(Value), colour = diabetic)) + 
  geom_boxplot() + 
  facet_wrap(~ Variable, scales = "free_y") + 
  theme_bw() + 
  labs(x = "Diabetic", y = "Value", 
       colour = "Diabetic")
```

 * Most of non-diabetic patients are younger (less than 30 years).
 * Diabetic patients have higher BMI, serum insulin, plasma glucose and thicker triceps.
 * Female diabetic patients are more likely to have had high number of pregnancies, or given birth to many children.
 
Based on the distributions of age, BMI, diabetes pedigree, plasma glucose, number of pregnancies and serum insulin, the two classes seem to be separable.

```{r}
# Check for highly correlated features
corrplot::corrplot(cor(dbTib[-9]))
```
 
All the features are lowly correlated. There's no multicollinearity.

# Model Training

I'll try 6 different algorithms i.e Logistic Regression, Naive Bayes classifier, KNN, Random Forest, XGBoost and ANN. Before training the models, I'll first split the data into training and test sets. The training set will be used to train and fine-tune the models with cross-validation, and the test sets will be used for model validation. 

```{r}
# Partition the data into training and test sets (use 75/25 split)
# Set random seed for reproducibility
set.seed(1234)

# Data partitioning
train_index <- createDataPartition(dbTib$diabetic, p = 0.75, list = FALSE)
# Assign 75% to training set
training_data <- dbTib[train_index, ]
# Assign the remaining 25% to test set
test_data <- dbTib[-train_index, ]
```

## Logistic Regression model

```{r}
# Define classification task
diabetesTask <- makeClassifTask(data = training_data, target = "diabetic")
# Define learner
logReg <- makeLearner("classif.logreg", predict.type = "prob")
```

```{r}
# Train the model
logRegModel <- train(logReg, diabetesTask)
```

```{r}
# Cross-validate model training process

# Specify resampling strategy
kFold <- makeResampleDesc(method = "RepCV", folds = 6, 
                          reps = 50, stratify = TRUE)
# Cross-validate
logRegCV <- resample(learner = logReg, task = diabetesTask, 
                             resampling = kFold, 
                             measures = list(acc, fpr, fnr), 
                     show.info = FALSE)
# View model results
logRegCV$aggr
```
 
The Logistic Regression model has a training accuracy of 78.86%, which is good. The model generalizes well. The model however, has a high false negative rate.


# Naive Bayes model

```{r}
# I already defined a classification task, I'll just define a learner for the model
naiveLearner <- makeLearner("classif.naiveBayes", predict.type = "prob")
# Train the model
bayesModel <- train(naiveLearner, diabetesTask)
```

```{r}
# Cross-validate the model building procedure

# Define resampling description
kFold <- makeResampleDesc(method = "RepCV", folds = 6, 
                          reps = 50, stratify = TRUE)
# Cross-validate
bayesCV <- resample(learner = naiveLearner, 
                    task = diabetesTask, 
                    resampling = kFold, 
                    measures = list(mmce, acc, fpr, fnr), 
                    show.info = FALSE)
# Check performance
bayesCV$aggr
```

The Naive Bayes model has a training accuracy of 78.68%. This model also has a high False Negative rate. The model performs slightly lower than the Logistic Regression model.


# KNN model

```{r}
# Make learner
knnLearner <- makeLearner("classif.knn")
```

```{r}
# Define hyperparameter space for tuning k
knnParamSpace <- makeParamSet(makeDiscreteParam("k", values = 1:15))
# Define tuning grid
gridSearch <- makeTuneControlGrid()
# Define CV for tuning
cvForTuning <- makeResampleDesc("RepCV", folds = 6, reps = 50)
```

```{r}
# Tune the model with cross-validation
tunedK <- tuneParams(knnLearner, task = diabetesTask, 
                     resampling = cvForTuning, 
                     par.set = knnParamSpace, 
                     control = gridSearch, 
                     measures = list(acc, fpr, fnr), 
                     show.info = FALSE)
# Obtain the optimal hyperparameter
tunedK$x
```

Optimal value of k = 7.

```{r}
# Check mmce value
tunedK$y
```

The model has a lower mmce value (0.16), implying a good performance (has a training accuracy of 84%). The KNN model performs better than the Logistic Regression and Naive Bayes models.

```{r}
# Visualize the tuning process
# Obtained model data
knnTuningData <- generateHyperParsEffectData(tunedK)
# Plot
plotHyperParsEffect(knnTuningData, x = "k", y = "acc.test.mean",
plot.type = "line") +
theme_bw()
```

Accuracy is highest at k = 7.

```{r}
# Set hyperparameters for the final model
tunedKnn <- setHyperPars(makeLearner("classif.knn"), 
                         par.vals = tunedK$x)
# Train the final model
tunedKnnModel <- train(tunedKnn, diabetesTask)
```


# Random Forest model

```{r}
# Define learner
rf <- makeLearner("classif.randomForest", predict.type = "prob")
# I'll continue using the task I defined earlier
```

```{r}
# Define hyperparameter space for tuning
rf_ParamSpace <- makeParamSet(makeIntegerParam("ntree", lower = 200, upper = 300), 
                                 makeIntegerParam("mtry", lower = 4, upper = 10), 
                                 makeIntegerParam("nodesize", lower = 3, upper = 25), 
                                 makeIntegerParam("maxnodes", lower = 5, upper = 20))
```

```{r}
# Define random search method with 200 iterations
randSearch <- makeTuneControlRandom(maxit = 200)
# Define a 6-fold CV strategy
cvForTuning <- makeResampleDesc("CV", iters = 6)
```

```{r}
# Start parallelization
parallelStartSocket(cpus = detectCores())

# Tune the hyperparameters using cross-validation
tuned_rf_Pars <- tuneParams(rf, task = diabetesTask, 
                            resampling = cvForTuning, 
                            par.set = rf_ParamSpace, 
                            control = randSearch, 
                            measures = list(acc, fpr, fnr), 
                            show.info = FALSE)
# Stop parallelization
parallelStop()
```

```{r}
# View CV results
tuned_rf_Pars
```

The RF model has a mean misclassifcation error rate of 0.1056, which is low and implies a good performance (training accuracy of 89.44%). The model performs better than the KNN model.

```{r}
# Set the optimal hyperparameters for the final model
tuned_rf <- setHyperPars(rf, par.vals = tuned_rf_Pars$x)
# Train the final model with the optimal hyperparameters
tuned_rf_Model <- train(tuned_rf, diabetesTask)
```

```{r}
# Check if there are enough trees in the Random Forest model

# First extract model data
rfModelData <- getLearnerModel(tuned_rf_Model)
# Plot
plot(rfModelData)
```

The mean out-of-bag error begins to stabilize early at about 100 trees. This implies that I have enough number of trees in the random forest. The positive class has high mean out-of-bag error rate.


# XGBoost

```{r}
# Define learner
XGB <- makeLearner("classif.xgboost", predict.type = "prob")
```
```{r}
# Define hyperparameter space for tuning
xgbParamSpace <- makeParamSet(
makeNumericParam("eta", lower = 0, upper = 1),
makeNumericParam("gamma", lower = 0, upper = 5),
makeIntegerParam("max_depth", lower = 1, upper = 10),
makeNumericParam("min_child_weight", lower = 1, upper = 10),
makeNumericParam("subsample", lower = 0.5, upper = 1),
makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
makeIntegerParam("nrounds", lower = 100, upper = 100))
```

```{r}
# Define hyperparameter search strategy
randSearch <- makeTuneControlRandom(maxit = 700)
# Make resampling description for CV
cvForTuning <- makeResampleDesc("CV", iters = 6)

# Tune the model
tunedXgbPars <- tuneParams(learner = XGB, task = diabetesTask, 
                           resampling = cvForTuning, 
                           par.set = xgbParamSpace, 
                           control = randSearch, 
                           measures = list(mmce, acc, fpr, fnr), 
                           show.info = FALSE)
# Check performance
tunedXgbPars$y
```

Mean misclassification error (0.0396) for XGBoost is great! A training accuracy 0.9604 is very good, though the model might be overfitting the training data.

```{r}
# Train the final model using optimal hyperparameters

# Set the optimal hyperparameters for the final model
tunedXgb <- setHyperPars(XGB, par.vals = tunedXgbPars$x)
# Train the final model
tunedXgbModel <- train(tunedXgb, diabetesTask)
```

```{r}
# Check if there are enough trees in the model

# Extract model data
xgbModelData <- getLearnerModel(tunedXgbModel)
# Plot
ggplot(xgbModelData$evaluation_log, aes(iter, train_logloss)) + 
  geom_line() + geom_point()
```

Training log loss stabilizes after 75 iterations and decreases steadily. I used enough trees.

# Neural Network

```{r}
# Define learner for the neural network 
nnet <- makeLearner("classif.nnet", predict.type = "prob")
```

```{r}
# Define hyperparameter space for tuning
nnetParam_set <- makeParamSet(
  makeIntegerParam("size", lower = 3, upper = 20), # Number of units in hidden layer
  makeNumericParam("decay", lower = 0.001, upper = 0.8)) # Weight decay
```

```{r}
# Define search strategy to use random search
randSearch <- makeTuneControlRandom(maxit = 150)

# Make resampling description for CV
cvForTuning <- makeResampleDesc("CV", iters = 6, stratify = TRUE)

# Set random seed for reproducibility
set.seed(1234)

# Start parallelization
parallelStartSocket(cpus = detectCores())

# Tune the model with cross-validation
tunedNnetPars <- tuneParams(learner = nnet, task = diabetesTask, 
                            resampling = cvForTuning, 
                            par.set = nnetParam_set, 
                            control = randSearch, 
                            measures = list(mmce, acc, fpr, fnr), 
                            show.info = FALSE)

# Stop parallelization
parallelStop()
```

```{r}
# View CV results
print(tunedNnetPars$x)
tunedNnetPars$y
```

The Neural Net performs better than Logistic Regression, Naive Bayes and KNN classifiers, but is outperformed by Random Forest and XGBoost. The Neural Net has a training accuracy of 85.67%. The optimal hyperparameters are size of 6 neurons and decay rate of 0.385.

```{r}
# Train the final model using optimal hyperparameters

# Set the best parameters for the final model
tunedNnet <- setHyperPars(nnet, par.vals = tunedNnetPars$x)

# Train the final model
tunedNnetModel <- train(tunedNnet, diabetesTask)
```

# Benchmark the model-building processes

```{r, warning=FALSE, results='hide'}
# First create a list of learners
learners = list(logReg, naiveLearner, tunedKnn, tuned_rf, tunedXgb, tunedNnet)

# Make a resampling description for benchmarking
benchCV <- makeResampleDesc("RepCV", folds = 5, reps = 20)

# Benchmark
bench <- benchmark(learners, diabetesTask, benchCV, 
                   show.info = FALSE, measures = list(acc, kappa))
```

```{r}
# View the benchmarking results
bench
```

According to this benchmarking results, XGBoost is likely to give me the best-performing model, with an accuracy of more than 90%. Random Forest also performs well.


# Model Evaluation

I'll use the two performing models to make predictions on test data (RF & XGB) and evaluate how my models would perform on unseen data.

```{r}
# Use the RF model to make predictions on test data
rf_Preds <- predict(tuned_rf_Model, newdata = test_data)
# Collect prediction
rf_Preds_data <- rf_Preds$data
```

```{r}
# Calculate confusion matrix
confusionMatrix(table(rf_Preds_data$truth, rf_Preds_data$response))
```

The Random Forest model has a validation accuracy of 89.33%, which is good. The model has a higher Specificity(0.9449) than Sensitivity(0.8058). The training accuracy is slightly higher than the validation accuracy, implying that the model didn't overfit the training data.

```{r}
# Calculate ROC AUC value
rf_Preds_data |> roc_auc(truth = truth, prob.Yes)
```

The ROC AUC value of 0.958 for the Random Forest model is very good, implying that the model fits the data very well.

```{r}
# Plot ROC curve
rf_Preds_data |> roc_curve(truth = truth, prob.Yes) |> autoplot()
```

ROC curve looks good, very steady and is closely approaching the top left corner where AUC value is 1.

```{r}
# Plot variable importance for the Random Forest model
vip(tuned_rf_Model)
```

The Random Forest model finds number of pregnancies as the most important predictor of diabetes, followed by age, BMI, serum insulin, plasma glucose and diabetes pedigree.

```{r}
# Use the XGBoost model to make predictions on test data
xgbPreds <- predict(tunedXgbModel, newdata = test_data)
# Collect prediction
xgbPreds_data <- xgbPreds$data
```

```{r}
# Calculate confusion matrix
confusionMatrix(table(xgbPreds_data$truth, xgbPreds_data$response))
```

Wow! 95.81% validation accuracy. The XGBoost model has an excellent performance.

 * Sensitivity and Specificity for the XGBoost model are all good, with Specificity being high. The trade-off between Sensitivity and Specificity metrics is small. The Sensitivity and Precision for this model are also good (93.9% and 93.52% respectively).
 
```{r}
# Calculate ROC AUC value
xgbPreds_data |> roc_auc(truth = truth, prob.Yes)
```

The XGBoost model has a ROC AUC value of 0.99, which is excellent. The model fits the data very well.

```{r}
# Plot ROC curve
xgbPreds_data |> roc_curve(truth = truth, prob.Yes) |> autoplot()
```

The curve is almost touching the top left corner, near 1.

```{r}
# Plot variable importance for the XGBoost model
vip(tunedXgbModel, type = "gain")
```

Based on information gain ratio score, number of pregnancies is the most important predictor of diabetes, followed by age, BMI, serum insulin, plasma glucose, triceps thickness, diastolic blood pressure and diabetes pedigree respectively.

* However, the main limitation of this analysis is that I did not handle class imbalance in the data. 

# References

Rhys, H. I. (2020). Machine learning with R, the tidyverse, and mlr. Manning Publications. <https://livebook.manning.com/book/machine-learning-with-r-the-tidyverse-and-mlr/about-this-book>

Tasin, I., Nabil, T. U., Islam, S., & Khan, R. (2022). Diabetes prediction using machine learning and explainable AI techniques. Healthcare Technology Letters, 10(1-2), 1-10. <https://pmc.ncbi.nlm.nih.gov/articles/PMC10107388/#htl212039-sec-0010>

